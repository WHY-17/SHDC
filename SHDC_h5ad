# SHDC with .h5ad format datasets.
import numpy as np
import heapq
import math
import scanpy as sc
import anndata
from dca.api import dca
from preprocess import read_dataset, normalize
import pp
from sklearn.cluster import KMeans
from sklearn import metrics
import umap



def Denosing(adata):
    adata = read_dataset(adata,
                     transpose=False,
                     test_split=False,
                     copy=True)

    sc.pp.filter_genes(adata, min_counts=1)
    dca(adata,threads=1)
    sc.pp.normalize_per_cell(adata)
    sc.pp.log1p(adata)
    sc.pp.pca(adata, n_comps=50)
    # sc.tl.pca(adata, n_comps=50, svd_solver='arpack')
    # sc.pp.neighbors(adata, n_neighbors=50, n_pcs=50)
    # sc.tl.umap(adata, min_dist=0.1, n_components=100)
    # X = adata.obsm['X_umap']
    X = adata.obsm['X_pca']
    y = np.array(adata.obs['Group'])
    return X, y


def Normalization_Data(X):
    for i in range(len(X)):
        std_i = np.std(X[i])
        average_i = np.average(X[i])
        for j in range(X.shape[1]):
            X[i,j] = (X[i,j]-average_i) / std_i
    return X


def Heart_Kernal_Function(A, c_1, c_2, t):
    H = math.exp(-1*((numpy.sum(numpy.square(A[c_1]-A[c_2])))/(2*t**2)))
    return H


def One_Row_Similarity_Matrix(A, row, t):
    S_row = numpy.zeros(shape=(len(A)), dtype=numpy.float16)
    for i in range(len(A)):
        if i == row:
            S_row[i] = 1
        elif i > row:
            S_row[i] = Heart_Kernal_Function(A, row, i, t)
        else:
            continue
    return S_row


def Similarity_Matrix(A, t):         #G represents the number of kernels
    import numpy
    S = numpy.zeros(shape=(len(A),len(A)),dtype=numpy.float16)
    ppservers = ()
    ncpus = 32
    job_server = pp.Server(ncpus, ppservers=ppservers)
    jobs = []
    for i in range(len(A)):
        row = i
        jobs.append(job_server.submit(One_Row_Similarity_Matrix, (A, row, t),
                                          (Heart_Kernal_Function,),('numpy','math')))
        job_server.print_stats()

    for j in range(len(jobs)):
        p = jobs[j]()
        S[j] = p
    for m in range(len(S)):
        for n in range(len(S)):
            if m > n:
                S[m,n] = S[n,m]
    return S


def Adjacency_Matrix(S, knn):                   #knn refer to the number of neighbors belong to cell i
    for i in range(len(S)):
        max_index = heapq.nlargest(knn,S[i])
        for j in range(len(S)):
            if S[i,j] in max_index:
                S[i,j] = S[i,j]
            else:
                S[i,j] = 0
    return S


if __name__ == "__main__":
    adata = anndata.read_h5ad('Zeisel.h5ad')
    X, y = Denosing(adata)
    X = Normalization_Data(X)
    t = 100
    S = Similarity_Matrix(X, t)
    knn = 50
    W = Adjacency_Matrix(S, knn)
    W_umap = umap.UMAP(n_neighbors=50, min_dist=0.001, n_components=3, metric='correlation').fit_transform(W)
    kmeans = KMeans(n_clusters=9)
    y_pred = kmeans.fit_predict(X)
    nmi = np.round(metrics.normalized_mutual_info_score(y, y_pred), 5)
    print("NMI_score:", nmi)
    ari = np.round(metrics.adjusted_rand_score(y, y_pred), 5)
    print("ARI_score:", ari)
    adata = sc.AnnData(X)
    adata.obs['Group'] = y_pred
    sc.pp.neighbors(adata, n_neighbors=50, n_pcs=10)
    sc.tl.umap(adata)
    sc.pl.umap(adata, color='Group', title='Zeisel')
